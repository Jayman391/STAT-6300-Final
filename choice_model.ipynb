{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:33:56.217017Z",
     "start_time": "2023-12-05T18:33:38.790889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0\n",
      "Time: 10\n",
      "Time: 20\n",
      "Time: 30\n",
      "Time: 40\n",
      "Time: 50\n",
      "Time: 60\n",
      "Time: 70\n",
      "Time: 80\n",
      "Time: 90\n",
      "X[0]= 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Name: 0, Length: 100, dtype: int64\n",
      "X.shape= (100, 10)\n",
      "X=     0  1  2  3  4  5  6  7  8  9\n",
      "0   0  1  0  0  1  0  0  1  0  0\n",
      "1   0  0  0  0  0  0  0  0  0  0\n",
      "2   0  1  0  0  1  0  0  0  0  0\n",
      "3   0  0  0  0  0  0  0  2  0  0\n",
      "4   0  0  0  0  0  0  0  0  0  0\n",
      ".. .. .. .. .. .. .. .. .. .. ..\n",
      "95  0  3  0  0  2  0  0  4  0  0\n",
      "96  0  1  0  0  2  0  0  6  0  0\n",
      "97  0  3  0  0  6  0  0  4  0  0\n",
      "98  0  4  0  0  4  0  1  3  0  0\n",
      "99  0  4  0  0  6  0  0  3  0  0\n",
      "\n",
      "[100 rows x 10 columns]\n",
      "X.shape= (100, 10)\n",
      "X=     0  1  2  3  4  5  6  7  8  9\n",
      "0   0  1  0  0  1  0  0  1  0  0\n",
      "1   0  0  0  0  0  0  0  0  0  0\n",
      "2   0  1  0  0  1  0  0  0  0  0\n",
      "3   0  0  0  0  0  0  0  2  0  0\n",
      "4   0  0  0  0  0  0  0  0  0  0\n",
      ".. .. .. .. .. .. .. .. .. .. ..\n",
      "95  0  3  0  0  2  0  0  4  0  0\n",
      "96  0  1  0  0  2  0  0  6  0  0\n",
      "97  0  3  0  0  6  0  0  4  0  0\n",
      "98  0  4  0  0  4  0  1  3  0  0\n",
      "99  0  4  0  0  6  0  0  3  0  0\n",
      "\n",
      "[100 rows x 10 columns]\n",
      "X.shape= (100, 10)\n",
      "X= 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Name: 0, Length: 100, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connorg0110/Library/Python/3.11/lib/python/site-packages/pymc/data.py:433: UserWarning: The `mutable` kwarg was not specified. Before v4.1.0 it defaulted to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default changed to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Use `pm.ConstantData`/`pm.MutableData` or pass `pm.Data(..., mutable=False/True)` to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/connorg0110/Library/Python/3.11/lib/python/site-packages/pymc/distributions/timeseries.py:558: UserWarning: Initial distribution not specified, defaulting to `Normal.dist(0, 100, shape=...)`. You can specify an init_dist manually to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 728\u001B[0m\n\u001B[1;32m    726\u001B[0m sim\u001B[38;5;241m.\u001B[39mrun()\n\u001B[1;32m    727\u001B[0m \u001B[38;5;66;03m#sim.community_regression_model()\u001B[39;00m\n\u001B[0;32m--> 728\u001B[0m \u001B[43msim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup_regression_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[15], line 677\u001B[0m, in \u001B[0;36mSimulation.group_regression_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    674\u001B[0m y_obs \u001B[38;5;241m=\u001B[39m pm\u001B[38;5;241m.\u001B[39mAR(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_obs\u001B[39m\u001B[38;5;124m'\u001B[39m, rho\u001B[38;5;241m=\u001B[39mphi, sigma\u001B[38;5;241m=\u001B[39msigma_ar,observed \u001B[38;5;241m=\u001B[39m X_data)\n\u001B[1;32m    676\u001B[0m \u001B[38;5;66;03m# Sampling\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m trace \u001B[38;5;241m=\u001B[39m \u001B[43mpm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtune\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdraws\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    678\u001B[0m posterior_predictive \u001B[38;5;241m=\u001B[39m pm\u001B[38;5;241m.\u001B[39msample_posterior_predictive(trace)\n\u001B[1;32m    679\u001B[0m prior_predictive \u001B[38;5;241m=\u001B[39m pm\u001B[38;5;241m.\u001B[39msample_prior_predictive()\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/sampling/mcmc.py:689\u001B[0m, in \u001B[0;36msample\u001B[0;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001B[0m\n\u001B[1;32m    686\u001B[0m         auto_nuts_init \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    688\u001B[0m initial_points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 689\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[43massign_step_methods\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethods\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSTEP_METHODS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nuts_sampler \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpymc\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(step, NUTS):\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/sampling/mcmc.py:239\u001B[0m, in \u001B[0;36massign_step_methods\u001B[0;34m(model, step, methods, step_kwargs)\u001B[0m\n\u001B[1;32m    231\u001B[0m         selected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m    232\u001B[0m             methods_list,\n\u001B[1;32m    233\u001B[0m             key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m method, var\u001B[38;5;241m=\u001B[39mrv_var, has_gradient\u001B[38;5;241m=\u001B[39mhas_gradient: method\u001B[38;5;241m.\u001B[39m_competence(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    234\u001B[0m                 var, has_gradient\n\u001B[1;32m    235\u001B[0m             ),\n\u001B[1;32m    236\u001B[0m         )\n\u001B[1;32m    237\u001B[0m         selected_steps\u001B[38;5;241m.\u001B[39msetdefault(selected, [])\u001B[38;5;241m.\u001B[39mappend(var)\n\u001B[0;32m--> 239\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minstantiate_steppers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/sampling/mcmc.py:140\u001B[0m, in \u001B[0;36minstantiate_steppers\u001B[0;34m(model, steps, selected_steps, step_kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m         args \u001B[38;5;241m=\u001B[39m step_kwargs\u001B[38;5;241m.\u001B[39mget(name, {})\n\u001B[1;32m    139\u001B[0m         used_keys\u001B[38;5;241m.\u001B[39madd(name)\n\u001B[0;32m--> 140\u001B[0m         step \u001B[38;5;241m=\u001B[39m \u001B[43mstep_class\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m         steps\u001B[38;5;241m.\u001B[39mappend(step)\n\u001B[1;32m    143\u001B[0m unused_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(step_kwargs)\u001B[38;5;241m.\u001B[39mdifference(used_keys)\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/step_methods/hmc/nuts.py:180\u001B[0m, in \u001B[0;36mNUTS.__init__\u001B[0;34m(self, vars, max_treedepth, early_max_treedepth, **kwargs)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mvars\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, max_treedepth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, early_max_treedepth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    123\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Set up the No-U-Turn sampler.\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \n\u001B[1;32m    125\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    `pm.sample` to the desired number of tuning steps.\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 180\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_treedepth \u001B[38;5;241m=\u001B[39m max_treedepth\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mearly_max_treedepth \u001B[38;5;241m=\u001B[39m early_max_treedepth\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/step_methods/hmc/base_hmc.py:109\u001B[0m, in \u001B[0;36mBaseHMC.__init__\u001B[0;34m(self, vars, scaling, step_scale, is_cov, model, blocked, potential, dtype, Emax, target_accept, gamma, k, t0, adapt_step_size, step_rand, **pytensor_kwargs)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mvars\u001B[39m \u001B[38;5;241m=\u001B[39m get_value_vars_from_user_vars(\u001B[38;5;28mvars\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model)\n\u001B[0;32m--> 109\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblocked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblocked\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpytensor_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madapt_step_size \u001B[38;5;241m=\u001B[39m adapt_step_size\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mEmax \u001B[38;5;241m=\u001B[39m Emax\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/step_methods/arraystep.py:164\u001B[0m, in \u001B[0;36mGradientSharedStep.__init__\u001B[0;34m(self, vars, model, blocked, dtype, logp_dlogp_func, **pytensor_kwargs)\u001B[0m\n\u001B[1;32m    161\u001B[0m model \u001B[38;5;241m=\u001B[39m modelcontext(model)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m logp_dlogp_func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogp_dlogp_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpytensor_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    166\u001B[0m     func \u001B[38;5;241m=\u001B[39m logp_dlogp_func\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/model/core.py:612\u001B[0m, in \u001B[0;36mModel.logp_dlogp_function\u001B[0;34m(self, grad_vars, tempered, **kwargs)\u001B[0m\n\u001B[1;32m    606\u001B[0m ip \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_point(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    607\u001B[0m extra_vars_and_values \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    608\u001B[0m     var: ip[var\u001B[38;5;241m.\u001B[39mname]\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_vars\n\u001B[1;32m    610\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m input_vars \u001B[38;5;129;01mand\u001B[39;00m var \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m grad_vars\n\u001B[1;32m    611\u001B[0m }\n\u001B[0;32m--> 612\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mValueGradFunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcosts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_vars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_vars_and_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/model/core.py:339\u001B[0m, in \u001B[0;36mValueGradFunction.__init__\u001B[0;34m(self, costs, grad_vars, extra_vars_and_values, dtype, casting, compute_grads, **kwargs)\u001B[0m\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extra_vars_shared[var\u001B[38;5;241m.\u001B[39mname] \u001B[38;5;241m=\u001B[39m shared\n\u001B[1;32m    337\u001B[0m     givens\u001B[38;5;241m.\u001B[39mappend((var, shared))\n\u001B[0;32m--> 339\u001B[0m cost \u001B[38;5;241m=\u001B[39m \u001B[43mrewrite_pregrad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcost\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compute_grads:\n\u001B[1;32m    342\u001B[0m     grads \u001B[38;5;241m=\u001B[39m pytensor\u001B[38;5;241m.\u001B[39mgrad(cost, grad_vars, disconnected_inputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pymc/pytensorf.py:1236\u001B[0m, in \u001B[0;36mrewrite_pregrad\u001B[0;34m(graph)\u001B[0m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrewrite_pregrad\u001B[39m(graph):\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Apply simplifying or stabilizing rewrites to graph that are safe to use\u001B[39;00m\n\u001B[1;32m   1234\u001B[0m \u001B[38;5;124;03m    pre-grad.\u001B[39;00m\n\u001B[1;32m   1235\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1236\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrewrite_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcanonicalize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstabilize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/graph/rewriting/utils.py:61\u001B[0m, in \u001B[0;36mrewrite_graph\u001B[0;34m(graph, include, custom_rewrite, clone, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m     fgraph \u001B[38;5;241m=\u001B[39m FunctionGraph(outputs\u001B[38;5;241m=\u001B[39moutputs, clone\u001B[38;5;241m=\u001B[39mclone)\n\u001B[1;32m     60\u001B[0m query_rewrites \u001B[38;5;241m=\u001B[39m optdb\u001B[38;5;241m.\u001B[39mquery(RewriteDatabaseQuery(include\u001B[38;5;241m=\u001B[39minclude, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n\u001B[0;32m---> 61\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mquery_rewrites\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrewrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfgraph\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m custom_rewrite:\n\u001B[1;32m     64\u001B[0m     custom_rewrite\u001B[38;5;241m.\u001B[39mrewrite(fgraph)\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/graph/rewriting/basic.py:121\u001B[0m, in \u001B[0;36mGraphRewriter.rewrite\u001B[0;34m(self, fgraph, *args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \n\u001B[1;32m    114\u001B[0m \u001B[38;5;124;03mThis is meant as a shortcut for the following::\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    118\u001B[0m \n\u001B[1;32m    119\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_requirements(fgraph)\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/graph/rewriting/basic.py:292\u001B[0m, in \u001B[0;36mSequentialGraphRewriter.apply\u001B[0;34m(self, fgraph)\u001B[0m\n\u001B[1;32m    290\u001B[0m nb_nodes_before \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(fgraph\u001B[38;5;241m.\u001B[39mapply_nodes)\n\u001B[1;32m    291\u001B[0m t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m--> 292\u001B[0m sub_prof \u001B[38;5;241m=\u001B[39m \u001B[43mrewriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfgraph\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    293\u001B[0m l\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m t0))\n\u001B[1;32m    294\u001B[0m sub_profs\u001B[38;5;241m.\u001B[39mappend(sub_prof)\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/graph/rewriting/basic.py:2428\u001B[0m, in \u001B[0;36mEquilibriumGraphRewriter.apply\u001B[0;34m(self, fgraph, start_from)\u001B[0m\n\u001B[1;32m   2426\u001B[0m nb \u001B[38;5;241m=\u001B[39m change_tracker\u001B[38;5;241m.\u001B[39mnb_imported\n\u001B[1;32m   2427\u001B[0m t_rewrite \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m-> 2428\u001B[0m node_rewriter_change \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_node\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2429\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_rewriter\u001B[49m\n\u001B[1;32m   2430\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2431\u001B[0m time_rewriters[node_rewriter] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m t_rewrite\n\u001B[1;32m   2432\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m node_rewriter_change:\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/graph/rewriting/basic.py:1922\u001B[0m, in \u001B[0;36mNodeProcessingGraphRewriter.process_node\u001B[0;34m(self, fgraph, node, node_rewriter)\u001B[0m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m node_rewriter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1922\u001B[0m     replacements \u001B[38;5;241m=\u001B[39m \u001B[43mnode_rewriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1924\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfailure_callback \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/graph/rewriting/basic.py:1082\u001B[0m, in \u001B[0;36mFromFunctionNodeRewriter.transform\u001B[0;34m(self, fgraph, node)\u001B[0m\n\u001B[1;32m   1077\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n\u001B[1;32m   1078\u001B[0m         node\u001B[38;5;241m.\u001B[39mop \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tracks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node\u001B[38;5;241m.\u001B[39mop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tracked_types)\n\u001B[1;32m   1079\u001B[0m     ):\n\u001B[1;32m   1080\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1082\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.11/lib/python/site-packages/pytensor/tensor/rewriting/basic.py:491\u001B[0m, in \u001B[0;36mlocal_alloc_sink_dimshuffle\u001B[0;34m(fgraph, node)\u001B[0m\n\u001B[1;32m    489\u001B[0m output_shape \u001B[38;5;241m=\u001B[39m node\u001B[38;5;241m.\u001B[39minputs[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m    490\u001B[0m num_dims_with_size_1_added_to_left \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 491\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(output_shape) \u001B[38;5;241m-\u001B[39m inp\u001B[38;5;241m.\u001B[39mndim):\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m extract_constant(output_shape[i], only_process_constants\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    493\u001B[0m         num_dims_with_size_1_added_to_left \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import floor\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import warnings\n",
    "import pandas as pd \n",
    "\n",
    "class Simulation:\n",
    "\n",
    "    ## Fixed hyperparameters\n",
    "\n",
    "    num_timesteps = 100\n",
    "\n",
    "    initial_users = 20\n",
    "    initial_groups = 10\n",
    "    initial_communities = 5\n",
    "\n",
    "    # group and community preferences\n",
    "    alpha_group_hyperparameter = 10\n",
    "    beta_group_hyperparameter = 10 \n",
    "\n",
    "    alpha_community_hyperparameter = 10\n",
    "    beta_community_hyperparameter = 10\n",
    "\n",
    "        \n",
    "    # Initialize lists to store users and groups\n",
    "    users = []\n",
    "    groups = []\n",
    "    communities = []\n",
    "\n",
    "\n",
    "    gis = {}\n",
    "    cis = {}\n",
    "    uis = {}\n",
    "\n",
    "    def __init__(self, user_growth_rate, interaction_threshold, new_group_rate, new_community_rate ) -> None:\n",
    "\n",
    "        self.user_growth_rate = user_growth_rate\n",
    "\n",
    "        self.interaction_threshold = interaction_threshold\n",
    "\n",
    "        self.new_group_rate = new_group_rate\n",
    "        self.new_group_join_chance = new_group_rate / 10\n",
    "\n",
    "        self.new_community_rate = new_community_rate\n",
    "        self.new_community_join_chance = new_community_rate / 10\n",
    "\n",
    "        self.same_community_interaction_ratio = new_community_rate * new_group_rate\n",
    "\n",
    "    class Community:\n",
    "        def __init__(self, simulation, group=None):\n",
    "            self.simulation = simulation \n",
    "            # Initialize a community with a list of users and groups\n",
    "            self.id = len(self.simulation.communities) + 1\n",
    "            self.groups = [group] if group else []\n",
    "            self.interactions = []\n",
    "\n",
    "    class Group:\n",
    "        def __init__(self, simulation):\n",
    "            self.simulation = simulation \n",
    "            # Initialize a group with an ID and a dictionary tgo track user interactions\n",
    "            self.id = len(self.simulation.groups) + 1\n",
    "            self.interactions = {}\n",
    "            self.community = None\n",
    "\n",
    "        def join_community(self, community):\n",
    "            community.groups.append(self)\n",
    "            self.community = community\n",
    "\n",
    "    class User:\n",
    "        def __init__(self, group_alpha, group_beta, community_alpha, community_beta):\n",
    "            # Initialize a user with ID, group memberships, interaction history, and Beta distribution preferences\n",
    "            self.id = None\n",
    "\n",
    "            self.groups = []\n",
    "            self.communities = []\n",
    "            self.interaction_history = []\n",
    "\n",
    "            self.group_preferences = stats.beta(group_alpha, group_beta)\n",
    "            self.community_preferences = stats.beta(community_alpha, community_beta)\n",
    "            self.updated_preferences = np.array([1])\n",
    "\n",
    "        def update_preferences(self):\n",
    "            # Update user's preferences based on group interactions\n",
    "            if not self.groups:\n",
    "                self.updated_preferences = np.array([1])\n",
    "                return\n",
    "            else:\n",
    "                #sort groups by number of interactions\n",
    "                self.groups.sort(key=lambda group: len(group.interactions))\n",
    "\n",
    "            total_size = sum([len(group.interactions) for group in self.groups])\n",
    "\n",
    "            # if size is 0, this must be the first iteration, return uniform\n",
    "            if total_size == 0:\n",
    "                self.ccdf = np.array([1])\n",
    "                return\n",
    "            else:\n",
    "                sizes = sorted([len(group.interactions) for group in self.groups])\n",
    "                self.ccdf = 1 - (np.cumsum(sizes) / total_size)\n",
    "\n",
    "            group_convolution = np.convolve(self.group_preferences.pdf(np.linspace(0, 1, len(self.groups))), self.ccdf , mode='same')\n",
    "\n",
    "            self.updated_preferences = np.convolve(group_convolution, self.community_preferences.pdf(np.linspace(0, 1, len(self.groups))), mode='same')\n",
    "\n",
    "            if np.isnan(self.updated_preferences).any() or np.sum(self.updated_preferences) == 0:\n",
    "                self.updated_preferences = np.array([1 / len(self.groups)] * len(self.groups))\n",
    "            else:\n",
    "                self.updated_preferences /= np.sum(self.updated_preferences)\n",
    "\n",
    "        def join_group(self, group):\n",
    "            # Add a group to the user's group list and set initial interactions to 0\n",
    "            self.groups.append(group)\n",
    "            group.interactions[self] = 0\n",
    "\n",
    "        def interact(self, group):\n",
    "            # Record an interaction with the specified group\n",
    "            group.interactions[self] = group.interactions.get(self, 0) + 1\n",
    "            self.interaction_history.append(group.id)\n",
    "\n",
    "    # Recalculate probabilities at every iteration or after any changes\n",
    "    def calculate_probabilities(self):\n",
    "        global community_relative_frequency, group_relative_frequency\n",
    "\n",
    "        community_relative_frequency = np.array([len(community.groups) for community in self.communities], dtype=float)\n",
    "        # if community_relative_frequency.sum() != 0:\n",
    "        community_relative_frequency += 1e-5  # Avoid division by zero\n",
    "        community_relative_frequency /= community_relative_frequency.sum()\n",
    "\n",
    "        group_relative_frequency = np.array([sum(group.interactions.values()) for group in self.groups], dtype=float)\n",
    "        # if group_relative_frequency.sum() != 0:\n",
    "        group_relative_frequency += 1e-5\n",
    "        group_relative_frequency /= group_relative_frequency.sum()\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "\n",
    "        # Initialize users\n",
    "        for i in range(self.initial_users):\n",
    "            self.users.append(\n",
    "                self.User(\n",
    "                    self.alpha_group_hyperparameter,\n",
    "                    self.beta_group_hyperparameter,\n",
    "                    self.alpha_community_hyperparameter,\n",
    "                    self.beta_community_hyperparameter,\n",
    "                )\n",
    "            )\n",
    "            self.users[-1].id = len(self.users)\n",
    "\n",
    "        # Initialize communities\n",
    "        for i in range(self.initial_communities):\n",
    "            self.communities.append(self.Community(self))\n",
    "\n",
    "        # Initialize groups\n",
    "        for i in range(self.initial_groups):\n",
    "            self.groups.append(self.Group(self))\n",
    "\n",
    "        # adding the first groups to each community so there is at least one group in each community\n",
    "        for i in range(len(self.communities)):\n",
    "            self.groups[i].join_community(self.communities[i])\n",
    "            # random chance for each user to join the first group of a new community\n",
    "            for user in self.users:\n",
    "                if np.random.random() < self.new_community_join_chance:\n",
    "                    user.join_group(self.groups[i])\n",
    "\n",
    "        # randomly adding the rest of the groups to communities\n",
    "        for group in self.groups[len(self.communities):]:\n",
    "            group.join_community(self.communities[np.random.randint(0, len(self.communities))])\n",
    "            for user in self.users:\n",
    "                if np.random.random() < self.new_group_join_chance:\n",
    "                    user.join_group(group)\n",
    "\n",
    "        # initialize dictionaries for each group, community, and user\n",
    "        for group in self.groups:\n",
    "            self.gis[group.id] = []\n",
    "        for community in self.communities:\n",
    "            self.cis[community.id] = []\n",
    "        for user in self.users:\n",
    "            self.uis[user.id] = []\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        # main loop\n",
    "        for time in range(self.num_timesteps):\n",
    "            if time % 10 == 0:\n",
    "                print(f\"Time: {time}\")\n",
    "            # Calculate probabilities\n",
    "            self.calculate_probabilities()\n",
    "\n",
    "            # Add new users\n",
    "            new_users_count = floor(np.random.exponential(self.user_growth_rate))\n",
    "            for i in range(new_users_count):\n",
    "                self.users.append(\n",
    "                    self.User(\n",
    "                        self.alpha_group_hyperparameter,\n",
    "                        self.beta_group_hyperparameter,\n",
    "                        self.alpha_community_hyperparameter,\n",
    "                        self.beta_community_hyperparameter,\n",
    "                    )\n",
    "                )\n",
    "                self.users[-1].id = len(self.users)\n",
    "\n",
    "            # Add new groups\n",
    "            new_groups_count = floor(np.random.exponential(self.new_group_rate))\n",
    "            for i in range(new_groups_count):\n",
    "                self.groups.append(self.Group(self))\n",
    "\n",
    "                # a new community always get made on the first time step\n",
    "                if time == 0:\n",
    "                    if new_groups_count == 0:\n",
    "                        self.groups.append(self.Group(self))\n",
    "                    self.groups[-1].join_community(self.communities[-1])\n",
    "                    self.communities[-1].groups.append(self.groups[-1])\n",
    "                else:\n",
    "                    # check if the new group forms a new community\n",
    "                    if np.random.random() < self.new_community_rate:\n",
    "                        self.communities.append(self.Community(self, self.groups[-1]))\n",
    "                        self.groups[-1].community = self.communities[-1]\n",
    "                        # each user has a chance to join the new community\n",
    "                        for user in self.users:\n",
    "                            if np.random.random() < self.new_community_join_chance:\n",
    "                                user.join_group(self.groups[-1])\n",
    "                        # users[np.random.randint(0, len(users))].join_group(groups[-1])\n",
    "                    else:\n",
    "                        # join a random community\n",
    "                        self.groups[-1].join_community(self.communities[np.random.randint(0, len(self.communities))])\n",
    "\n",
    "            # Updating dictionaries with new groups, communities, and users\n",
    "            # and setting their initial values to 0\n",
    "            for group in self.groups:\n",
    "                if group.id not in self.gis:\n",
    "                    self.gis[group.id] = [0]\n",
    "                self.gis[group.id].append(0)\n",
    "            for community in self.communities:\n",
    "                if community.id not in self.cis:\n",
    "                    self.cis[community.id] = [0]\n",
    "                self.cis[community.id].append(0)\n",
    "            for user in self.users:\n",
    "                if user.id not in self.uis:\n",
    "                    self.uis[user.id] = [0]\n",
    "                self.uis[user.id].append(0)\n",
    "\n",
    "            # Add new users to groups\n",
    "            for user in self.users:\n",
    "                self.calculate_probabilities()\n",
    "                # if there are groups for the user to join that they aren't in\n",
    "                if len(user.groups) < len(self.groups):\n",
    "                    # join a group\n",
    "                    if np.random.random() < self.new_group_join_chance:\n",
    "                        user.join_group(self.groups[np.random.choice(len(self.groups), p=group_relative_frequency)])\n",
    "\n",
    "            # Interact with groups\n",
    "            for user in self.users:\n",
    "                user.update_preferences()\n",
    "                interacted_groups = []\n",
    "                if np.random.uniform() < self.interaction_threshold and user.groups:\n",
    "                    # print(user.updated_preferences)\n",
    "                    group = np.random.choice(user.groups, p=user.updated_preferences)\n",
    "                    user.interact(group)\n",
    "                    self.gis[group.id][-1] += 1\n",
    "                    self.cis[group.community.id][-1] += 1\n",
    "                    self.uis[user.id][-1] += 1\n",
    "\n",
    "                    # potential bonus interactions within another group in the same community\n",
    "                    if group.community:\n",
    "                        while True:\n",
    "                            if np.random.uniform() < self.same_community_interaction_ratio:\n",
    "                                community = group.community                    \n",
    "                                group = np.random.choice(community.groups)\n",
    "                                user.interact(group)\n",
    "                                self.gis[group.id][-1] += 1\n",
    "                                self.cis[group.community.id][-1] += 1\n",
    "                                self.uis[user.id][-1] += 1\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "            # Update user preferences\n",
    "            for user in self.users:\n",
    "                if user.groups:\n",
    "                    user.update_preferences()\n",
    "                    if user.id == 0:\n",
    "                        print(user.updated_preferences)\n",
    "                        print(user.group_preferences.pdf(np.linspace(0, 1, len(user.groups))))\n",
    "                else:\n",
    "                    user.updated_preferences = np.array([1])\n",
    "\n",
    "    def plot(self, sim_number):        \n",
    "        directory_name = f\"{self.user_growth_rate}_{self.interaction_threshold}_{self.new_group_rate}_{self.new_community_rate}/{sim_number}\"\n",
    "        os.makedirs(directory_name, exist_ok=True)\n",
    "\n",
    "        c_sum = []\n",
    "        c_sum_labels = []\n",
    "        for i in range(len(self.communities)):\n",
    "            temp_sum = [0] * self.num_timesteps\n",
    "            c_vals = np.cumsum(self.cis[i+1])\n",
    "            # add the values starting from the back\n",
    "            for j, val in enumerate(reversed(c_vals)):\n",
    "                temp_sum[-1-j] = val\n",
    "            c_sum.append(temp_sum)\n",
    "            c_sum_labels.append(list(self.cis.keys())[i])\n",
    "\n",
    "        # print the final value for each community\n",
    "        for c in c_sum_labels[:5]:\n",
    "            print(c, c_sum[c_sum_labels.index(c)][-1])\n",
    "            \n",
    "        # finding the labels for the 5 largest communities\n",
    "        top_5 = []\n",
    "        top_5_labels = []\n",
    "        for i in range(5):\n",
    "            max_val = 0\n",
    "            max_index = 0\n",
    "            for j in range(len(c_sum)):\n",
    "                if c_sum[j][-1] > max_val and c_sum_labels[j] not in top_5_labels:\n",
    "                    max_val = c_sum[j][-1]\n",
    "                    max_index = j\n",
    "            top_5.append(c_sum[max_index])\n",
    "            top_5_labels.append(c_sum_labels[max_index])\n",
    "\n",
    "        for i in range(len(c_sum)):\n",
    "            if c_sum_labels[i] in top_5_labels:\n",
    "                plt.plot(c_sum[i][:len(c_sum[i])], label=f\"C{i+1}\")\n",
    "            else:\n",
    "                plt.plot(c_sum[i][:len(c_sum[i])], label=None)\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Cumulative Interactions\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.title(\"Cumulative Interactions of Each Community Over Time\")\n",
    "        plt.savefig(f\"{directory_name}/community_interaction_growth.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Scatter plot for final amount of interactions for each community\n",
    "        c_sum_final = []\n",
    "        c_sum_final_labels = []\n",
    "        for i in range(len(self.communities)):\n",
    "            c_sum_final.append(c_sum[i][-1])\n",
    "            c_sum_final_labels.append(list(self.cis.keys())[i])\n",
    "\n",
    "        plt.scatter(c_sum_final_labels, c_sum_final)\n",
    "        plt.xlabel(\"Community\")\n",
    "        plt.ylabel(\"Final Cumulative Interactions\")\n",
    "        plt.title(\"Final Cumulative Interactions of Each Community\")\n",
    "        plt.savefig(f\"{directory_name}/final_community_interactions.png\")\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "        g_sum = []\n",
    "        g_sum_labels = []\n",
    "        for i in range(1, len(self.groups)):\n",
    "            temp_sum = [0] * self.num_timesteps\n",
    "            g_vals = np.cumsum(self.gis[i])\n",
    "            # add the values starting from the back\n",
    "            for j, val in enumerate(reversed(g_vals)):\n",
    "                try:\n",
    "                    temp_sum[j] = val\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            temp_sum = temp_sum[::-1]\n",
    "            g_sum.append(temp_sum)\n",
    "            g_sum_labels.append(list(self.gis.keys())[i])\n",
    "\n",
    "        # print the final value for each group\n",
    "        for g in g_sum_labels[:5]:\n",
    "            print(g, g_sum[g_sum_labels.index(g)][-1])\n",
    "\n",
    "        # finding the labels for the 5 largest groups\n",
    "        top_5 = []\n",
    "        top_5_labels = []\n",
    "        for i in range(5):\n",
    "            max_val = 0\n",
    "            max_index = 0\n",
    "            for j in range(len(g_sum)):\n",
    "                if g_sum[j][-1] > max_val and g_sum_labels[j] not in top_5_labels:\n",
    "                    max_val = g_sum[j][-1]\n",
    "                    max_index = j\n",
    "            top_5.append(g_sum[max_index])\n",
    "            top_5_labels.append(g_sum_labels[max_index])\n",
    "\n",
    "        for i in range(len(g_sum)):\n",
    "            if g_sum_labels[i] in top_5_labels:\n",
    "                plt.plot(g_sum[i], label=f\"G{i+1}\")\n",
    "            else:\n",
    "                plt.plot(g_sum[i], label=None)\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Cumulative Interactions\")\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.ylim(bottom=1)\n",
    "        plt.title(\"Cumulative Interactions of Each Group Over Time\")\n",
    "        plt.savefig(f\"{directory_name}/group_interaction_growth.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Scatter plot for final amount of interactions for each group\n",
    "        g_sum_final = []\n",
    "        g_sum_final_labels = []\n",
    "        for i in range(len(g_sum)):\n",
    "            g_sum_final.append(g_sum[i][-1])\n",
    "            g_sum_final_labels.append(list(self.gis.keys())[i])\n",
    "\n",
    "        plt.scatter(g_sum_final_labels, g_sum_final)\n",
    "        plt.xlabel(\"Group\")\n",
    "        plt.ylabel(\"Final Cumulative Interactions\")\n",
    "        plt.title(\"Final Cumulative Interactions of Each Group\")\n",
    "        plt.savefig(f\"{directory_name}/final_group_interactions.png\")\n",
    "        plt.close()\n",
    "\n",
    "        for u in self.uis:\n",
    "            self.uis[u] = self.uis[u][:self.num_timesteps]\n",
    "\n",
    "        # plotting total amount of interactions for each user\n",
    "        u_sum = []\n",
    "        u_sum_labels = []\n",
    "        for i in range(1, len(self.users)):\n",
    "            temp_sum = [0] * self.num_timesteps\n",
    "            u_vals = np.cumsum(self.uis[i])\n",
    "            # add the values starting from the back\n",
    "            for j, val in enumerate(reversed(u_vals)):\n",
    "                temp_sum[j] = val\n",
    "\n",
    "            temp_sum = temp_sum[::-1]\n",
    "            u_sum.append(temp_sum)\n",
    "            u_sum_labels.append(list(self.uis.keys())[i])\n",
    "\n",
    "        # print the final value for each user\n",
    "        for u in u_sum_labels[:5]:\n",
    "            print(u, u_sum[u_sum_labels.index(u)][-1])\n",
    "\n",
    "        # finding the labels for the 5 largest users\n",
    "        top_5 = []\n",
    "        top_5_labels = []\n",
    "\n",
    "        for i in range(5):\n",
    "            max_val = 0\n",
    "            max_index = 0\n",
    "            for j in range(len(u_sum)):\n",
    "                if u_sum[j][-1] > max_val and u_sum_labels[j] not in top_5_labels:\n",
    "                    max_val = u_sum[j][-1]\n",
    "                    max_index = j\n",
    "            top_5.append(u_sum[max_index])\n",
    "            top_5_labels.append(u_sum_labels[max_index])\n",
    "\n",
    "        # Scatter plot for final amount of interactions for each user\n",
    "        u_sum_final = []\n",
    "        u_sum_final_labels = []\n",
    "        for i in range(len(u_sum)):\n",
    "            u_sum_final.append(u_sum[i][-1])\n",
    "            u_sum_final_labels.append(list(self.uis.keys())[i])\n",
    "\n",
    "        plt.scatter(u_sum_final_labels, u_sum_final)\n",
    "        plt.xlabel(\"User\")\n",
    "        plt.ylabel(\"Final Cumulative Interactions\")\n",
    "        plt.title(\"Cumulative Interactions of Each User\")\n",
    "        plt.savefig(f\"{directory_name}/final_user_interactions.png\")\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    def write_data(self, sim_number):\n",
    "        directory_name = f\"{self.user_growth_rate}_{self.interaction_threshold}_{self.new_group_rate}_{self.new_community_rate}/{sim_number}\"\n",
    "        os.makedirs(directory_name, exist_ok=True)\n",
    "\n",
    "        # Write User Interactions to CSV\n",
    "        with open(f\"{directory_name}/user_interactions.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for row in self.uis:\n",
    "                writer.writerow([row] + self.uis[row])\n",
    "\n",
    "            file.close()\n",
    "        \n",
    "        # Write Group Interactions to CSV\n",
    "        with open(f\"{directory_name}/group_interactions.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for row in self.gis:\n",
    "                writer.writerow([row] + self.gis[row])\n",
    "\n",
    "            file.close()\n",
    "\n",
    "        # Write Community Interactions to CSV\n",
    "        with open(f\"{directory_name}/community_interactions.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for row in self.cis:\n",
    "                writer.writerow([row] + self.cis[row])\n",
    "\n",
    "            file.close()\n",
    "\n",
    "\n",
    "        with open(f\"{directory_name}/simulation_data.csv\", 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['num_users', 'num_groups', 'num_communities', 'num_interactions'])\n",
    "            writer.writerow([len(self.users), len(self.groups), len(self.communities), sum([len(group.interactions) for group in self.groups])])\n",
    "\n",
    "            file.close()\n",
    "\n",
    "        print(\"Data written to CSV files.\")\n",
    "\n",
    "    def community_regression_model(self):\n",
    "\n",
    "        y = []\n",
    "        X = []\n",
    "        c_sum = 0\n",
    "\n",
    "        for community in range(len(self.communities)):\n",
    "            df = pd.DataFrame(self.cis[community + 1]) - pd.DataFrame(self.cis[community + 1]).shift(1)\n",
    "            df.fillna(0, inplace=True)\n",
    "            X.append(df)\n",
    "            c = pd.DataFrame(self.cis[community + 1])\n",
    "            c_sum += c.sum()            \n",
    "\n",
    "        # get rid of third dimension of X\n",
    "        X = np.array(X)\n",
    "        X = X.reshape(X.shape[0], X.shape[1])\n",
    "\n",
    "            \n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "        y = X.iloc[:,-1]\n",
    "        y.fillna(0, inplace=True)\n",
    "\n",
    "        X = X.iloc[:,:-1]\n",
    "        X.fillna(0, inplace=True)\n",
    "\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        model = pm.Model()\n",
    "\n",
    "        with model:\n",
    "            try:\n",
    "                # Data\n",
    "                group_index = pm.Data('group_index', np.arange(len(self.communities)), dims='community')\n",
    "                X_data = pm.Data('X_data', X.T)\n",
    "                y_data = pm.Data('y_data', y)\n",
    "\n",
    "                # Priors\n",
    "                rho = pm.Normal('phi', mu=0, sigma=c_sum/len(self.communities), dims='community')\n",
    "                sigma_ar = pm.Exponential('sigma_ar', lam=1)\n",
    "\n",
    "                # AR Model\n",
    "                ar = pm.AR('ar', rho=rho, sigma=sigma_ar, dims='community')\n",
    "\n",
    "\n",
    "                # Likelihood\n",
    "                sigma = pm.Exponential('sigma', lam=1)\n",
    "                y_obs = pm.Normal('y_obs', mu=ar, sigma=sigma, observed=y_data, dims='community')\n",
    "\n",
    "                # Sampling\n",
    "                trace = pm.sample(tune=1000, draws=1000)\n",
    "                posterior_predictive = pm.sample_posterior_predictive(trace)\n",
    "                prior_predictive = pm.sample_prior_predictive()\n",
    "            \n",
    "            except pm.exceptions.SamplingError:\n",
    "                warnings.warn('SamplingError: Skipping this group')\n",
    "\n",
    "        prior = prior_predictive['prior_predictive']['y_obs']\n",
    "\n",
    "        cs = [[] for _ in range(len(self.communities))]\n",
    "\n",
    "        for chain in range(prior.shape[0]):\n",
    "            for draw in range(prior.shape[1]):\n",
    "                for c in range(prior.shape[2]):\n",
    "                    cs[c].append(prior[chain,draw,c])\n",
    "\n",
    "        for c in cs:\n",
    "            plt.hist(c)\n",
    "        plt.show()\n",
    "\n",
    "        pred = posterior_predictive['posterior_predictive']['y_obs'].to_numpy()\n",
    "\n",
    "        shape = pred.shape\n",
    "        groups1 = shape[2]\n",
    "\n",
    "        # Initialize list of lists for each community\n",
    "        preds = [[] for _ in range(groups1)]\n",
    "\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                for k in range(groups1):\n",
    "                    preds[k].append(pred[i, j, k])\n",
    "\n",
    "        for k in range(groups1):\n",
    "\n",
    "            plt.hist(preds[k], bins=50, alpha=0.5)\n",
    "            plt.axvline(y[k], color='k', linestyle='dashed', linewidth=1)\n",
    "            plt.title('Community Interaction Predictions')\n",
    "            plt.xlabel(\"Number of interactions\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "            \n",
    "        print(az.summary(trace))\n",
    "\n",
    "    \n",
    "    def group_regression_model(self):\n",
    "\n",
    "        y = []\n",
    "        X = pd.DataFrame()\n",
    "        g_sum = 0\n",
    "\n",
    "\n",
    "        for group in range(len(self.groups)):\n",
    "            df = pd.DataFrame(self.gis[group + 1]) \n",
    "            df.fillna(0, inplace=True)\n",
    "            if len(df)<self.num_timesteps:\n",
    "                df = df.reindex(range(self.num_timesteps), fill_value=0)\n",
    "            X[group] = df.squeeze()            \n",
    "\n",
    "        # get rid of third dimension of X\\\n",
    "            \n",
    "        #print(X.sum().idxmax())\n",
    "        #print(X[X.sum().idxmax()])\n",
    "        \n",
    "        #X =  X[X.sum().idxmax()]\n",
    "        X = np.array(X)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        #print(X)\n",
    "        #print(\"len(groups)\", len(self.groups))\n",
    "\n",
    "        \n",
    "        print('X[0]=', X[0])\n",
    "        print('X.shape=', X.shape)\n",
    "        print('X=', X)\n",
    "        print('X.shape=', X.shape)\n",
    "\n",
    "        for group in range(len(self.groups)):\n",
    "            \n",
    "            model = pm.Model()\n",
    "            X1 = X[group]\n",
    "            print('X=', X)\n",
    "            print('X.shape=', X.shape)\n",
    "            X1 = X1.T\n",
    "            print('X=', X1)\n",
    "            y = X1.iloc[-1]\n",
    "            X1 = X1.iloc[:-1]\n",
    "            X1 = X1.to_numpy()\n",
    "            X1.flatten()\n",
    "            \n",
    "            with model:\n",
    "                try:\n",
    "                    # Data\n",
    "                    group_index = pm.Data('group_index', np.arange(len(self.groups)), dims='group')\n",
    "                    X_data = pm.Data('X_data', X1)\n",
    "                    y_data = pm.Data('y_data', y)\n",
    "    \n",
    "                    # Priors\n",
    "                    phi = pm.Normal('phi', mu=0, sigma=2)\n",
    "                    sigma_ar = pm.Exponential('sigma_ar', lam=1)\n",
    "    \n",
    "                    # AR Model\n",
    "                    #ar = pm.AR('ar', rho=phi, sigma=sigma_ar, dims='group',observed = X_data)\n",
    "    \n",
    "    \n",
    "                    # Likelihood\n",
    "                    #sigma = pm.Exponential('sigma',lam=1)\n",
    "                    #y_obs = pm.Normal('y_obs', mu=ar, sigma=sigma, observed=y_data, dims='group')\n",
    "                    y_obs = pm.AR('y_obs', rho=phi, sigma=sigma_ar,observed = X_data)\n",
    "    \n",
    "                    # Sampling\n",
    "                    trace = pm.sample(tune=1000, draws=1000)\n",
    "                    posterior_predictive = pm.sample_posterior_predictive(trace)\n",
    "                    prior_predictive = pm.sample_prior_predictive()\n",
    "                    print(az.summary(trace))\n",
    "                \n",
    "                except pm.exceptions.SamplingError:\n",
    "                    warnings.warn('SamplingError: Skipping this group')\n",
    "\n",
    "        prior = prior_predictive['prior_predictive']['y_obs']\n",
    "\n",
    "        gs = [[] for _ in range(len(self.groups))]\n",
    "\n",
    "        for chain in range(prior.shape[0]):\n",
    "            for draw in range(prior.shape[1]):\n",
    "                for g in range(prior.shape[2]):\n",
    "                    gs[g].append(prior[chain,draw,g])\n",
    "\n",
    "        for g in gs:\n",
    "            plt.hist(g)\n",
    "        plt.show()\n",
    "\n",
    "        pred = posterior_predictive['posterior_predictive']['y_obs'].to_numpy()\n",
    "\n",
    "        shape = pred.shape\n",
    "        groups1 = shape[2]\n",
    "\n",
    "        # Initialize list of lists for each community\n",
    "        preds = [[] for _ in range(groups1)]\n",
    "\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                for k in range(groups1):\n",
    "                    preds[k].append(pred[i, j, k])\n",
    "        \"\"\"\n",
    "        for k in range(groups1):\n",
    "\n",
    "            plt.hist(preds[k], bins=50, alpha=0.5)\n",
    "            plt.axvline(y[k], color='k', linestyle='dashed', linewidth=1)\n",
    "            plt.title('Group Interaction Predictions')\n",
    "            plt.xlabel(\"Number of interactions\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "            \n",
    "        #print(az.summary(trace))\n",
    "        \n",
    "        \n",
    "sim = Simulation(1, 0.2, .2, 0.25)\n",
    "sim.initialize()\n",
    "sim.run()\n",
    "#sim.community_regression_model()\n",
    "sim.group_regression_model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T20:54:31.062181Z",
     "start_time": "2023-12-04T20:54:31.056545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T20:56:39.811108Z",
     "start_time": "2023-12-04T20:56:39.794738Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'groups' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# multinomial logit model for group interactions\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m num_groups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[43mgroups\u001B[49m)\n\u001B[1;32m      3\u001B[0m y \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m X \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mNameError\u001B[0m: name 'groups' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# multinomial logit model for group interactions\n",
    "num_groups = len(groups)\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "for user in users:\n",
    "    y.append(user.interaction_history[-1])\n",
    "    # every interaction but the last one\n",
    "    X.append(np.bincount(user.interaction_history[:-1], minlength=num_groups))\n",
    "\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "y = pd.DataFrame(y)\n",
    "y = y.iloc[:,0]\n",
    "\n",
    "# drop first column if sum is 0\n",
    "if X.iloc[:,0].sum() == 0:\n",
    "    X = X.iloc[:,1:]\n",
    "    if num_groups != X.shape[1]:\n",
    "        num_groups -= (X.shape[1] - num_groups)\n",
    "\n",
    "for row in range(len(X)):\n",
    "    X.iloc[row] = X.iloc[row] / X.iloc[row].sum()\n",
    "\n",
    "X.fillna(0, inplace=True)\n",
    "X = (X - X.mean()) / X.std()\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "model = pm.Model()\n",
    "\n",
    "\n",
    "with model:\n",
    "    try:\n",
    "\n",
    "        indices = pm.Data('index', list(range(len(X))), dims='user')\n",
    "        # Define your data within the model\n",
    "        X_data = pm.Data('X_data', X)\n",
    "        y_data = pm.Data('y_data', y)\n",
    "\n",
    "        # Model parameters\n",
    "        \n",
    "        alpha = pm.Exponential('alpha', lam=np.max(group_relative_frequency), shape=num_groups)\n",
    "\n",
    "        beta_mu = pm.Normal('beta_mu', mu=1, sigma=10, shape=num_groups)\n",
    "        beta_sd = pm.TruncatedNormal('beta_sd', lower=0, mu=5, sigma=2.5, shape=num_groups)\n",
    "\n",
    "        beta = pm.Normal('beta', mu=beta_mu, sigma=beta_sd, shape=(num_groups, num_groups))\n",
    "\n",
    "        # Computing mu\n",
    "        mu = alpha + pm.math.dot(X_data, beta) \n",
    "\n",
    "        # A numerically stable softmax\n",
    "        mu_max = pm.math.max(mu, axis=1, keepdims=True)\n",
    "        p = pm.Deterministic('p', pm.math.exp(mu - mu_max) / pm.math.sum(pm.math.exp(mu - mu_max), axis=1, keepdims=True))\n",
    "\n",
    "        # Categorical distribution for observed data\n",
    "        y_obs = pm.Categorical('y_obs', p=p, observed=y_data, dims='user')\n",
    "\n",
    "        # Sampling\n",
    "        trace = pm.sample(tune=5000, draws=5000)\n",
    "        posterior_predictive = pm.sample_posterior_predictive(trace)\n",
    "        prior_predictive = pm.sample_prior_predictive()\n",
    "\n",
    "    except pm.exceptions.SamplingError:\n",
    "        model.debug(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(az.summary(trace))\n",
    "\n",
    "y_pred = np.rint(posterior_predictive['posterior_predictive']['y_obs'].mean(axis=(0,1))) \n",
    "\n",
    "plt.scatter(y, y_pred)\n",
    "plt.xlabel('Actual Group ID')\n",
    "plt.ylabel('Predicted Group ID')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y, y_pred - y)\n",
    "plt.xlabel('Actual Group ID')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()\n",
    "\n",
    "print(az.summary(trace))\n",
    "\n",
    "plt.hist(y_pred, bins=num_groups, alpha=0.5, label='Predicted')\n",
    "plt.hist(y,  bins=num_groups, alpha=0.5, label='Actual')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Group ID\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "samples = posterior_predictive['posterior_predictive']['y_obs'].to_numpy()\n",
    "users = [[] for _ in range(samples.shape[2])]\n",
    "for chain in range(samples.shape[0]):\n",
    "  for draw in range(samples.shape[1]):\n",
    "    for user in range(samples.shape[2]):\n",
    "      users[user].append(samples[chain, draw, user])\n",
    "\n",
    "\n",
    "# Define the grid layout\n",
    "num_users = len(users)\n",
    "num_rows = int(num_users**0.5)  # Number of rows in the grid\n",
    "num_cols = (num_users + num_rows - 1) // num_rows  # Number of columns in the grid\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "# Iterate through users and plot histograms\n",
    "for user_idx, ax in enumerate(axes.flat):\n",
    "    if user_idx < num_users:\n",
    "        user_data = users[user_idx]\n",
    "        actual_interaction = y[user_idx]\n",
    "\n",
    "        ax.hist(user_data, bins=num_groups)\n",
    "        ax.axvline(actual_interaction, color='r', linestyle='dashed', linewidth=1)\n",
    "\n",
    "        ax.set_title(f'User {user_idx + 1}')\n",
    "        ax.set_xlabel('Interactions')\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for user_idx in range(num_users, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flat[user_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
